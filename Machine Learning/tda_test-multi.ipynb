{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install giotto-tda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a warmup, let's generate some noisy signals with a constant SNR of 17.98. As shown in Table 1 of the article, this corresponds to an $R$ value of 0.65. By picking the upper end of the interval, we place ourselves in a favorable scenario and, thus, can gain a sense for what the best possible performance is for our time series classifier. We pick a small number of samples to make the computations run fast, but in practice would scale this by 1-2 orders of magnitude as done in the original article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import getcwd\n",
    "\n",
    "mypath = getcwd()\n",
    "dataset1_imag = np.load(mypath+'/Data/7-16-24/OneSubcarrierHighDataRate.npy').imag\n",
    "dataset1_real = np.load(mypath+'/Data/7-16-24/OneSubcarrierHighDataRate.npy').real\n",
    "\n",
    "dataset2_imag = np.load(mypath+'/Data/7-16-24/OneSubcarrierLowDataRate.npy').imag\n",
    "dataset2_real = np.load(mypath+'/Data/7-16-24/OneSubcarrierLowDataRate.npy').real\n",
    "\n",
    "dataset3_imag = np.load(mypath+'/Data/7-16-24/TwoSubcarriersHighDataRate.npy').imag\n",
    "dataset3_real = np.load(mypath+'/Data/7-16-24/TwoSubcarriersHighDataRate.npy').real\n",
    "\n",
    "dataset4_imag = np.load(mypath+'/Data/7-16-24/TwoSubcarriersLowDataRate.npy').imag\n",
    "dataset4_real = np.load(mypath+'/Data/7-16-24/TwoSubcarriersLowDataRate.npy').real\n",
    "\n",
    "print(dataset1_imag.shape)\n",
    "print(dataset2_imag.shape)\n",
    "print(dataset3_imag.shape)\n",
    "print(dataset4_imag.shape)\n",
    "\n",
    "def find_norm(data_real,data_imag,target_length):\n",
    "    \n",
    "    sample_num,data_length,category_num = data_real.shape\n",
    "    \n",
    "    dataset = np.zeros([sample_num,target_length,category_num])\n",
    "    \n",
    "    for i in range(category_num):\n",
    "        for j in range(sample_num):\n",
    "            dataset[j,:,i] = [np.sqrt(data_real[j,indx,i]**2+data_imag[j,indx,i]**2) for indx in range(target_length)]\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "len1 = 900\n",
    "len2 = 3500\n",
    "\n",
    "dataset1_norm = find_norm(dataset1_real,dataset1_imag,len1)\n",
    "\n",
    "dataset2_norm = find_norm(dataset2_real,dataset2_imag,len2)\n",
    "\n",
    "dataset3_norm = find_norm(dataset3_real,dataset3_imag,len1)\n",
    "\n",
    "dataset4_norm = find_norm(dataset4_real,dataset4_imag,len2)\n",
    "\n",
    "\n",
    "# dataset = np.zeros([dataset_real.shape[0]*dataset_real.shape[2],dataset_real.shape[1]-30])\n",
    "# label = np.zeros([dataset_real.shape[0]*dataset_real.shape[2]])\n",
    "# for i in range(dataset_real.shape[2]):\n",
    "#     for j in range(dataset_real.shape[0]-30):\n",
    "#         dataset[j+dataset_real.shape[0]*i,:] = [np.sqrt(dataset_real[j,indx,i]**2+dataset_imag[j,indx,i]**2) for indx in range(dataset_real.shape[1]-30)]\n",
    "#         label[j+dataset_real.shape[0]*i] = i\n",
    "\n",
    "# print(f\"Number of noisy signals: {len(dataset)}\")\n",
    "# print(dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(dataset3_norm[200,:,3])\n",
    "# plt.figure(2)\n",
    "# plt.plot(abs(rx_data))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num,_,category_num = dataset1_norm.shape\n",
    "dataset = np.zeros([sample_num*category_num,2*(len1+len2)])\n",
    "label = np.zeros([sample_num*category_num])\n",
    "for i in range(category_num):\n",
    "    for j in range(sample_num):\n",
    "        dataset[j+sample_num*i,:] =np.concatenate((np.concatenate((dataset1_norm[j,:,i],dataset2_norm[j,:,i])),np.concatenate((dataset3_norm[j,:,i],dataset4_norm[j,:,i]))))\n",
    "        label[j+sample_num*i] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.shape)\n",
    "plt.figure(1)\n",
    "plt.plot(dataset[1000,0:8500])\n",
    "# plt.figure(2)\n",
    "# plt.plot(abs(rx_data))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store moving averages\n",
    "window_size = 5\n",
    "dataset_avg= np.zeros([dataset.shape[0],dataset.shape[1]])\n",
    "# Loop through the array to consider\n",
    "# every window of size 3\n",
    "for j in range(dataset.shape[0]):\n",
    "    i=0\n",
    "    while i < dataset.shape[1] - window_size +1:\n",
    "\n",
    "        # Store elements from i to i+window_size\n",
    "        # in list to get the current window\n",
    "        window = dataset[j][i : i + window_size]\n",
    "\n",
    "        # Calculate the average of current window\n",
    "        window_average = sum(window) / window_size\n",
    "\n",
    "        # Store the average of current\n",
    "        # window in moving average list\n",
    "        dataset_avg[j][i] = dataset[j][i]-window_average\n",
    "        if abs(dataset_avg[j][i])>0.015:\n",
    "                dataset_avg[j][i]=np.sign(dataset_avg[j][i])*0.015\n",
    "\n",
    "        # Shift window to right by one position\n",
    "        i += 1\n",
    "#print(dataset_avg[4,:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(dataset_avg[1000,0:8500])\n",
    "# plt.figure(2)\n",
    "# plt.plot(abs(rx_data))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's visualise the two different types of time series that we wish to classify: one that is pure noise vs. one that is composed of noise plus an embedded gravitational wave signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(range(len(dataset_avg[0,:]))), y=dataset_avg[0,:], mode=\"lines\", name=\"noise\"),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(range(len(dataset_avg[84,:]))), y=dataset_avg[84,:], mode=\"lines\", name=\"noise\"),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make two observations:\n",
    "1. It is hard to distinguish the signal by eye,\n",
    "2. The signal features some regularity or periodicity.\n",
    "\n",
    "Both observations lead us to examining the _**Takens embedding**_ of the signal $s(t)$, in order to pick up the recurrent structure. Indeed, if $f$ is sampled from a dynamical system with a non-trivial recurrent structure, then, for appropriate parameters, the image by the embedding will have non-trivial topology.\n",
    "\n",
    "More formally,, we extract a sequence of vectors in $\\mathbb{R}^{d}$ of the form\n",
    "\n",
    "$$\n",
    "TD_{d,\\tau} s : \\mathbb{R} \\to \\mathbb{R}^{d}\\,, \\qquad t \\to \\begin{bmatrix}\n",
    "           s(t) \\\\\n",
    "           s(t + \\tau) \\\\\n",
    "           s(t + 2\\tau) \\\\\n",
    "           \\vdots \\\\\n",
    "           s(t + (d-1)\\tau)\n",
    "         \\end{bmatrix},\n",
    "$$\n",
    "where $d$ is the embedding dimension and $\\tau$ is the time delay. The quantity $(d-1)\\tau$ is known as the \"window size\" and the difference between $t_{i+1}$ and $t_i$ is called the stride.\n",
    "\n",
    "Let's examine what the time delay embedding of a pure gravitational wave signal looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.time_series import SingleTakensEmbedding\n",
    "embedding_dimension = 5\n",
    "embedding_time_delay = 5\n",
    "stride = 2\n",
    "\n",
    "embedder = SingleTakensEmbedding(\n",
    "    parameters_type=\"search\", n_jobs=-1, time_delay=embedding_time_delay, dimension=embedding_dimension, stride=stride\n",
    ")\n",
    "\n",
    "y_gw_embedded = embedder.fit_transform(dataset_avg[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use PCA to project our high-dimensional space to 3-dimensions for visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from gtda.plotting import plot_point_cloud\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "y_gw_embedded_pca = pca.fit_transform(y_gw_embedded)\n",
    "\n",
    "plot_point_cloud(y_gw_embedded_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot we can see that the decaying periodic signal generated by a black hole merger emerges as a _spiral_ in the time delay embedding space! For contrast, let's compare this to one of the pure noise time series in our sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 5\n",
    "embedding_time_delay = 5\n",
    "stride = 2\n",
    "\n",
    "embedder = SingleTakensEmbedding(\n",
    "    parameters_type=\"search\", n_jobs=5, time_delay=embedding_time_delay, dimension=embedding_dimension, stride=stride\n",
    ")\n",
    "\n",
    "y_noise_embedded = embedder.fit_transform(dataset_avg[11,:])\n",
    "print(y_noise_embedded.shape)\n",
    "pca = PCA(n_components=3)\n",
    "y_noise_embedded_pca = pca.fit_transform(y_noise_embedded)\n",
    "\n",
    "plot_point_cloud(y_noise_embedded_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, pure noise resembles a high-dimensional ball in the time delay embedding space. Let's see if we can use persistent homology to tease apart which time series contain a gravitational wave signal versus those that don't. To do so we will adapt the strategy from the original article:\n",
    "\n",
    "1. Generate 200-dimensional time delay embeddings of each time series\n",
    "2. Use PCA to reduce the time delay embeddings to 3-dimensions\n",
    "3. Use the Vietoris-Rips construction to calculate persistence diagrams of $H_0$ and $H_1$ generators\n",
    "4. Extract feature vectors using persistence entropy\n",
    "5. Train a binary classifier on the topological features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the topological feature generation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do steps 1 and 2 by using the following ``giotto-tda`` tools:\n",
    "\n",
    "- The ``TakensEmbedding`` transformer – instead of ``SingleTakensEmbedding`` – which will transform each time series in ``noisy_signals`` separately and return a collection of point clouds;\n",
    "- ``CollectionTransformer``, which is a convenience \"meta-estimator\" for applying the same PCA to each point cloud resulting from step 1.\n",
    "\n",
    "Using the ``Pipeline`` class from ``giotto-tda``, we can chain all operations up to and including step 4 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn\n",
    "\n",
    "\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.diagrams import PersistenceEntropy, Scaler\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.metaestimators import CollectionTransformer\n",
    "from gtda.pipeline import Pipeline\n",
    "from gtda.time_series import TakensEmbedding\n",
    "\n",
    "embedding_dimension = 5\n",
    "embedding_time_delay = 5\n",
    "stride = 2\n",
    "\n",
    "embedder = TakensEmbedding(time_delay=embedding_time_delay,\n",
    "                           dimension=embedding_dimension,\n",
    "                           stride=stride)\n",
    "\n",
    "batch_pca = CollectionTransformer(PCA(n_components=5), n_jobs=-1)\n",
    "\n",
    "persistence = VietorisRipsPersistence(homology_dimensions=[0, 1,2], n_jobs=-1)\n",
    "\n",
    "scaling = Scaler()\n",
    "\n",
    "entropy = PersistenceEntropy(normalize=True, nan_fill_value=-10)\n",
    "\n",
    "\n",
    "steps = [(\"embedder\", embedder),\n",
    "         (\"pca\", batch_pca),\n",
    "         (\"persistence\", persistence),\n",
    "         (\"scaling\", scaling)]\n",
    "         #(\"entropy\", entropy)]\n",
    "topological_transfomer = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_avg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = topological_transfomer.fit_transform(dataset_avg)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.time_series import SingleTakensEmbedding\n",
    "\n",
    "embedder = SingleTakensEmbedding(time_delay=5, dimension=5)\n",
    "xt= embedder.fit_transform(dataset_avg[100])\n",
    "xt=xt[None,:,:]\n",
    "print(xt.shape)\n",
    "xt = persistence.fit_transform(xt)\n",
    "xt = scaling.fit_transform(xt)\n",
    "persistence.plot(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt= embedder.fit_transform(dataset_avg[301])\n",
    "xt=xt[None,:,:]\n",
    "print(xt.shape)\n",
    "xt = persistence.fit_transform(xt)\n",
    "xt = scaling.fit_transform(xt)\n",
    "persistence.plot(xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final step, let's train a simple classifier on our topological features. As usual we create training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    dataset_avg, label, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then fit and evaluate our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "\n",
    "def print_scores(fitted_model):\n",
    "    res = {\n",
    "        \"Accuracy on train:\": accuracy_score(fitted_model.predict(X_train), y_train),\n",
    "        # \"ROC AUC on train:\": roc_auc_score(\n",
    "        #     y_train, fitted_model.predict_proba(X_train)[:, 1]\n",
    "        # ),\n",
    "        \"Accuracy on valid:\": accuracy_score(fitted_model.predict(X_valid), y_valid),\n",
    "        # \"ROC AUC on valid:\": roc_auc_score(\n",
    "        #     y_valid, fitted_model.predict_proba(X_valid)[:, 1]\n",
    "        # ),\n",
    "    }\n",
    "\n",
    "    for k, v in res.items():\n",
    "        print(k, round(v, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print_scores(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "model = svm.SVC(kernel='rbf')\n",
    "model.fit(X_train, y_train)\n",
    "print_scores(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "model = RandomForestClassifier(max_depth=2, random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "print_scores(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    features, label, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple baseline, this model is not too bad - it outperforms the deep learning baseline in the article which typically fares little better than random on the raw data. However, the combination of deep learning and persistent homology is where significant performance gains are seen - we leave this as an exercise to the intrepid reader!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from gtda.diagrams import PersistenceEntropy\n",
    "from sklearn.pipeline import Pipeline\n",
    "from gtda.diagrams import Amplitude\n",
    "# Listing all metrics we want to use to extract diagram amplitudes\n",
    "metric_list = [\n",
    "    {\"metric\": \"bottleneck\", \"metric_params\": {}},\n",
    "    {\"metric\": \"wasserstein\", \"metric_params\": {\"p\": 1}},\n",
    "    {\"metric\": \"wasserstein\", \"metric_params\": {\"p\": 2}},\n",
    "    {\"metric\": \"landscape\", \"metric_params\": {\"p\": 1, \"n_layers\": 1, \"n_bins\": 100}},\n",
    "    {\"metric\": \"landscape\", \"metric_params\": {\"p\": 1, \"n_layers\": 2, \"n_bins\": 100}},\n",
    "    {\"metric\": \"landscape\", \"metric_params\": {\"p\": 2, \"n_layers\": 1, \"n_bins\": 100}},\n",
    "    {\"metric\": \"landscape\", \"metric_params\": {\"p\": 2, \"n_layers\": 2, \"n_bins\": 100}},\n",
    "    {\"metric\": \"betti\", \"metric_params\": {\"p\": 1, \"n_bins\": 100}},\n",
    "    {\"metric\": \"betti\", \"metric_params\": {\"p\": 2, \"n_bins\": 100}},\n",
    "    {\"metric\": \"heat\", \"metric_params\": {\"p\": 1, \"sigma\": 1.6, \"n_bins\": 100}},\n",
    "    {\"metric\": \"heat\", \"metric_params\": {\"p\": 1, \"sigma\": 3.2, \"n_bins\": 100}},\n",
    "    {\"metric\": \"heat\", \"metric_params\": {\"p\": 2, \"sigma\": 1.6, \"n_bins\": 100}},\n",
    "    {\"metric\": \"heat\", \"metric_params\": {\"p\": 2, \"sigma\": 3.2, \"n_bins\": 100}},\n",
    "]\n",
    "\n",
    "#\n",
    "feature_union = make_union(\n",
    "    *[PersistenceEntropy(nan_fill_value=-1)]\n",
    "    + [Amplitude(**metric, n_jobs=-1) for metric in metric_list]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tda = feature_union.fit_transform(features)\n",
    "X_train_tda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_tda, label, test_size=0.2, random_state=42\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "\n",
    "def print_scores(fitted_model):\n",
    "    res = {\n",
    "        \"Accuracy on train:\": accuracy_score(fitted_model.predict(X_train), y_train),\n",
    "        # \"ROC AUC on train:\": roc_auc_score(\n",
    "        #     y_train, fitted_model.predict_proba(X_train)[:, 1]\n",
    "        # ),\n",
    "        \"Accuracy on valid:\": accuracy_score(fitted_model.predict(X_valid), y_valid),\n",
    "        # \"ROC AUC on valid:\": roc_auc_score(\n",
    "        #     y_valid, fitted_model.predict_proba(X_valid)[:, 1]\n",
    "        # ),\n",
    "    }\n",
    "\n",
    "    for k, v in res.items():\n",
    "        print(k, round(v, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "model = svm.SVC(kernel='rbf')\n",
    "model.fit(X_train, y_train)\n",
    "print_scores(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print_scores(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "model = RandomForestClassifier(max_depth=3, random_state=10)\n",
    "model.fit(X_train, y_train)\n",
    "print_scores(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python-3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
